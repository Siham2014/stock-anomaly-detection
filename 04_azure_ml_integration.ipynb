{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cce0781-95ca-4f83-adc5-2f40c81b25f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ INSTALLATION DES PACKAGES AZURE ML...\n",
      "‚úÖ Packages install√©s!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [33 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\HH\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "      main()\n",
      "    File \"C:\\Users\\HH\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\HH\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 112, in get_requires_for_build_wheel\n",
      "      backend = _build_backend()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\HH\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 77, in _build_backend\n",
      "      obj = import_module(mod_path)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\HH\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "      return _bootstrap._gcd_import(name[level:], package, level)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "    File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "    File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "    File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "    File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "    File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "    File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "    File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "    File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "    File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "    File \"C:\\Users\\HH\\AppData\\Local\\Temp\\pip-build-env-myinm5nd\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n",
      "      import setuptools.version\n",
      "    File \"C:\\Users\\HH\\AppData\\Local\\Temp\\pip-build-env-myinm5nd\\overlay\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n",
      "      import pkg_resources\n",
      "    File \"C:\\Users\\HH\\AppData\\Local\\Temp\\pip-build-env-myinm5nd\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2172, in <module>\n",
      "      register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "# Cellule 1 - Installation packages\n",
    "print(\"üì¶ INSTALLATION DES PACKAGES AZURE ML...\")\n",
    "!pip install azureml-core azureml-dataset-runtime --upgrade --quiet\n",
    "print(\"‚úÖ Packages install√©s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe42d7c7-eb77-4920-8bbf-1bbdb9796480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó CONNEXION √Ä AZURE ML...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HH\\anaconda3\\Lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "C:\\Users\\HH\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "C:\\Users\\HH\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connect√© √†: stock-anomaly-ml-workspace\n",
      "üìç R√©gion: eastus\n",
      "üìä Resource Group: stock-anomaly-rg\n"
     ]
    }
   ],
   "source": [
    "# Cellule 2 - Connexion √† Azure ML\n",
    "print(\"üîó CONNEXION √Ä AZURE ML...\")\n",
    "from azureml.core import Workspace, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(f\"‚úÖ Connect√© √†: {ws.name}\")\n",
    "    print(f\"üìç R√©gion: {ws.location}\")\n",
    "    print(f\"üìä Resource Group: {ws.resource_group}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "567e4b2a-8bd1-49a5-a6fd-0d00f8a5a16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ TEST ALTERNATIF DU DATA ASSET...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Method download: This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "2025-11-18 16:46:49.326558 | ActivityCompleted: Activity=download, HowEnded=Failure, Duration=0.0 [ms], Info = {'activity_id': 'fdd5b52c-663e-487a-9567-c5efad6a62e1', 'activity_name': 'download', 'activity_type': 'InternalCall', 'app_name': 'TabularDataset', 'source': 'azureml.dataset', 'version': '1.60.0.post1', 'dataprepVersion': '5.4.1', 'sparkVersion': '', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': '', 'completionStatus': 'Success', 'durationMs': 0.0}, Exception=TypeError; TabularDataset.download() missing 1 required positional argument: 'stream_column'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Asset trouv√©: stock_anomaly_training_data (v1)\n",
      "üîÑ Tentative de chargement alternatif...\n",
      "üì• T√©l√©chargement du fichier depuis Azure ML...\n",
      "‚ùå Erreur: TabularDataset.download() missing 1 required positional argument: 'stream_column'\n",
      "\n",
      "üîÑ Option de secours: Utilisation du fichier local...\n",
      "‚úÖ Fichier local charg√©: (1000, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>162.429993</td>\n",
       "      <td>164.960007</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>164.660004</td>\n",
       "      <td>163.785767</td>\n",
       "      <td>45390100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>GOOG</td>\n",
       "      <td>105.769997</td>\n",
       "      <td>109.629997</td>\n",
       "      <td>104.815002</td>\n",
       "      <td>108.900002</td>\n",
       "      <td>108.900002</td>\n",
       "      <td>34684200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>283.209991</td>\n",
       "      <td>292.079987</td>\n",
       "      <td>282.029999</td>\n",
       "      <td>291.600006</td>\n",
       "      <td>289.226227</td>\n",
       "      <td>29770300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Ticker        Open        High         Low       Close  \\\n",
       "0  2023-04-06   AAPL  162.429993  164.960007  162.000000  164.660004   \n",
       "1  2023-04-06   GOOG  105.769997  109.629997  104.815002  108.900002   \n",
       "2  2023-04-06   MSFT  283.209991  292.079987  282.029999  291.600006   \n",
       "\n",
       "    Adj Close      Volume  \n",
       "0  163.785767  45390100.0  \n",
       "1  108.900002  34684200.0  \n",
       "2  289.226227  29770300.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cellule 3 CORRIG√âE - Test alternatif du Data Asset\n",
    "print(\"üéØ TEST ALTERNATIF DU DATA ASSET...\")\n",
    "\n",
    "try:\n",
    "    # R√©cup√©rer le Data Asset\n",
    "    dataset = Dataset.get_by_name(ws, name='stock_anomaly_training_data')\n",
    "    print(f\"‚úÖ Data Asset trouv√©: {dataset.name} (v{dataset.version})\")\n",
    "    \n",
    "    # M√©thode alternative de chargement\n",
    "    print(\"üîÑ Tentative de chargement alternatif...\")\n",
    "    \n",
    "    # Option 1: T√©l√©chargement local puis chargement\n",
    "    print(\"üì• T√©l√©chargement du fichier depuis Azure ML...\")\n",
    "    local_path = dataset.download(target_path='.', overwrite=True)\n",
    "    print(f\"‚úÖ Fichier t√©l√©charg√©: {local_path}\")\n",
    "    \n",
    "    # Chargement depuis le fichier local\n",
    "    import glob\n",
    "    csv_files = glob.glob(\"**/stock_data_cleaned_mlops.csv\", recursive=True)\n",
    "    if csv_files:\n",
    "        df = pd.read_csv(csv_files[0])\n",
    "        print(f\"üéâ DONN√âES CHARG√âES: {df.shape}\")\n",
    "        \n",
    "        # V√©rification\n",
    "        print(f\"\\nüìä V√âRIFICATIONS:\")\n",
    "        print(f\"‚Ä¢ Colonnes: {df.columns.tolist()}\")\n",
    "        print(f\"‚Ä¢ Tickers: {df['Ticker'].unique().tolist()}\")\n",
    "        print(f\"‚Ä¢ P√©riode: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "        \n",
    "        # Aper√ßu\n",
    "        print(f\"\\nüëÄ APER√áU:\")\n",
    "        display(df.head(3))\n",
    "    else:\n",
    "        print(\"‚ùå Fichier non trouv√© apr√®s t√©l√©chargement\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur: {e}\")\n",
    "    \n",
    "    # Option de secours: Utiliser le fichier local original\n",
    "    print(\"\\nüîÑ Option de secours: Utilisation du fichier local...\")\n",
    "    try:\n",
    "        df = pd.read_csv('stock_data.csv')\n",
    "        print(f\"‚úÖ Fichier local charg√©: {df.shape}\")\n",
    "        display(df.head(3))\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Erreur fichier local: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923ab683-e2f8-4b07-bab5-e81c2008f43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ ENREGISTREMENT DES R√âSULTATS D'ANOMALIES...\n",
      "üìä R√©sultats locaux charg√©s: (905, 8)\n",
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Validating arguments.\n",
      "Arguments validated.\n",
      "'overwrite' is set to True. Any file already present in the target will be overwritten.\n",
      "Uploading files from 'C:/Users/HH/AppData/Local/Temp/tmpl_q8u7b2' to 'managed-dataset/ed9c2597-da92-4747-823d-5f62d6f61ca2/'\n",
      "Successfully uploaded file to datastore.\n",
      "Creating and registering a new dataset.\n",
      "Successfully created and registered a new dataset.\n",
      "‚úÖ R√©sultats enregistr√©s: stock_anomaly_detection_results\n",
      "üìà 905 anomalies sauvegard√©es\n"
     ]
    }
   ],
   "source": [
    "# Cellule 4 SIMPLIFI√âE - Enregistrement des r√©sultats\n",
    "print(\"üíæ ENREGISTREMENT DES R√âSULTATS D'ANOMALIES...\")\n",
    "\n",
    "try:\n",
    "    # Charger les r√©sultats locaux\n",
    "    anomaly_results = pd.read_csv('anomaly_detection_results.csv')\n",
    "    print(f\"üìä R√©sultats locaux charg√©s: {anomaly_results.shape}\")\n",
    "    \n",
    "    # Enregistrement direct sans Data Asset complexe\n",
    "    results_dataset_name = \"stock_anomaly_detection_results\"\n",
    "    \n",
    "    # M√©thode simple d'enregistrement\n",
    "    from azureml.core import Dataset\n",
    "    \n",
    "    results_dataset = Dataset.Tabular.register_pandas_dataframe(\n",
    "        dataframe=anomaly_results,\n",
    "        target=ws.get_default_datastore(),\n",
    "        name=results_dataset_name,\n",
    "        description=\"R√©sultats de d√©tection d'anomalies - Isolation Forest\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ R√©sultats enregistr√©s: {results_dataset.name}\")\n",
    "    print(f\"üìà {len(anomaly_results)} anomalies sauvegard√©es\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur enregistrement: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4043fdaa-97eb-4313-a072-f4c05f0636c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ SAUVEGARDE DU MOD√àLE LOCAL...\n",
      "‚úÖ Mod√®le sauvegard√©: isolation_forest_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Cellule 1 - Sauvegarde du mod√®le\n",
    "print(\"üíæ SAUVEGARDE DU MOD√àLE LOCAL...\")\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Cr√©er et sauvegarder le mod√®le\n",
    "model = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
    "X_dummy = np.random.randn(100, 5)\n",
    "model.fit(X_dummy)\n",
    "\n",
    "joblib.dump(model, 'isolation_forest_model.joblib')\n",
    "print(\"‚úÖ Mod√®le sauvegard√©: isolation_forest_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd24072a-41f4-4881-9c5a-2adf209f87cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù CR√âATION DU SCRIPT DE SCORING...\n",
      "‚úÖ Script cr√©√©: score.py\n",
      "üìÅ Fichiers pr√™ts pour Azure ML:\n",
      "   ‚Ä¢ isolation_forest_model.joblib\n",
      "   ‚Ä¢ score.py\n"
     ]
    }
   ],
   "source": [
    "# Cellule 2 - Cr√©ation du script de scoring\n",
    "print(\"üìù CR√âATION DU SCRIPT DE SCORING...\")\n",
    "\n",
    "scoring_script = \"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # Charger le mod√®le depuis Azure ML\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'isolation_forest_model.joblib')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        # Convertir les donn√©es d'entr√©e\n",
    "        data = json.loads(raw_data)\n",
    "        \n",
    "        # Extraire les features\n",
    "        features = np.array(data['data']).reshape(-1, 5)\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        predictions = model.predict(features)\n",
    "        scores = model.decision_function(features)\n",
    "        \n",
    "        # R√©sultats\n",
    "        results = {\n",
    "            'anomaly_predictions': predictions.tolist(),\n",
    "            'anomaly_scores': scores.tolist(),\n",
    "            'is_anomaly': [1 if p == -1 else 0 for p in predictions],\n",
    "            'message': 'D√©tection d\\'anomalies r√©ussie',\n",
    "            'anomalies_count': sum(1 for p in predictions if p == -1)\n",
    "        }\n",
    "        \n",
    "        return json.dumps(results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_result = {'error': str(e)}\n",
    "        return json.dumps(error_result)\n",
    "\"\"\"\n",
    "\n",
    "with open('score.py', 'w') as f:\n",
    "    f.write(scoring_script)\n",
    "\n",
    "print(\"‚úÖ Script cr√©√©: score.py\")\n",
    "print(\"üìÅ Fichiers pr√™ts pour Azure ML:\")\n",
    "print(\"   ‚Ä¢ isolation_forest_model.joblib\")\n",
    "print(\"   ‚Ä¢ score.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c61a6-9c96-4ade-af11-27379cd74511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa50461-9eb0-4352-a843-abc2349ec464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
